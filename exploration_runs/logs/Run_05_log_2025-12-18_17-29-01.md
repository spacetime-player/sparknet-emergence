
================================================================================
SPARKNET EXPLORER - CONTINUOUS EXPLORATION TASK
================================================================================
Goal: Explore 2D space driven by curiosity and novelty
================================================================================

Run timestamp: 2025-12-18_17-29-01
Log file: exploration_runs/logs\run_log_2025-12-18_17-29-01.md

Using device: cuda

Starting exploration...
Initial position: [0. 0.]


Step 500/5000
  Position: [-0.9926, 1.0000]
  Exploring: False
  Novelty: 0.0394
  Curiosity: 0.0004
  Total Reward: 0.0116
  Exploration Rate: 0.2714
  State Space Coverage: 5.0% (500 states)
  Diversity: 0.2434

Step 1000/5000
  Position: [-1.0000, 1.0000]
  Exploring: False
  Novelty: 0.0284
  Curiosity: 0.0004
  Total Reward: 0.0140
  Exploration Rate: 0.2456
  State Space Coverage: 10.0% (1000 states)
  Diversity: 0.2124

============================================================
Homeostasis: Establishing desirable parameter ranges
============================================================
  hidden_layers.0.weight:
    Mean: -0.047925, Std: 0.403149
    Desirable range: [-0.854223, 0.758373]
  hidden_layers.0.bias:
    Mean: 0.006352, Std: 0.385171
    Desirable range: [-0.763990, 0.776694]
  hidden_layers.3.weight:
    Mean: -0.001902, Std: 0.051831
    Desirable range: [-0.105563, 0.101759]
  hidden_layers.3.bias:
    Mean: -0.005719, Std: 0.052148
    Desirable range: [-0.110015, 0.098578]
  hidden_layers.6.weight:
    Mean: -0.001967, Std: 0.036580
    Desirable range: [-0.075128, 0.071193]
  hidden_layers.6.bias:
    Mean: 0.000748, Std: 0.043578
    Desirable range: [-0.086407, 0.087903]
  output_layer.weight:
    Mean: 0.001131, Std: 0.049546
    Desirable range: [-0.097960, 0.100222]
  output_layer.bias:
    Mean: 0.045127, Std: 0.065210
    Desirable range: [-0.085292, 0.175547]
  state_embedder.0.weight:
    Mean: -0.000390, Std: 0.050799
    Desirable range: [-0.101987, 0.101208]
  state_embedder.0.bias:
    Mean: 0.012458, Std: 0.051852
    Desirable range: [-0.091247, 0.116163]
  state_embedder.2.weight:
    Mean: 0.000799, Std: 0.051130
    Desirable range: [-0.101462, 0.103059]
  state_embedder.2.bias:
    Mean: 0.000880, Std: 0.050546
    Desirable range: [-0.100213, 0.101972]
  curiosity_module.forward_model.0.weight:
    Mean: 0.000630, Std: 0.099401
    Desirable range: [-0.198172, 0.199433]
  curiosity_module.forward_model.0.bias:
    Mean: -0.015592, Std: 0.092364
    Desirable range: [-0.200320, 0.169136]
  curiosity_module.forward_model.3.weight:
    Mean: -0.003879, Std: 0.036364
    Desirable range: [-0.076608, 0.068850]
  curiosity_module.forward_model.3.bias:
    Mean: -0.007558, Std: 0.036397
    Desirable range: [-0.080353, 0.065237]
  curiosity_module.forward_model.6.weight:
    Mean: 0.001528, Std: 0.049354
    Desirable range: [-0.097180, 0.100237]
  curiosity_module.forward_model.6.bias:
    Mean: 0.000055, Std: 0.050706
    Desirable range: [-0.101358, 0.101467]
  curiosity_module.inverse_model.0.weight:
    Mean: 0.000184, Std: 0.074154
    Desirable range: [-0.148125, 0.148492]
  curiosity_module.inverse_model.0.bias:
    Mean: -0.004805, Std: 0.070065
    Desirable range: [-0.144934, 0.135325]
  curiosity_module.inverse_model.3.weight:
    Mean: -0.001181, Std: 0.040609
    Desirable range: [-0.082399, 0.080037]
  curiosity_module.inverse_model.3.bias:
    Mean: 0.028061, Std: 0.028678
    Desirable range: [-0.029295, 0.085416]
============================================================


Step 1500/5000
  Position: [-1.0000, 1.0000]
  Exploring: False
  Novelty: 0.0418
  Curiosity: 0.0005
  Total Reward: 0.0738
  Exploration Rate: 0.2222
  State Space Coverage: 15.0% (1500 states)
  Diversity: 0.2208
  Parameter Health: 22/22 healthy

Step 2000/5000
  Position: [-1.0000, 1.0000]
  Exploring: False
  Novelty: 0.0409
  Curiosity: 0.0002
  Total Reward: 0.0818
  Exploration Rate: 0.2011
  State Space Coverage: 20.0% (2000 states)
  Diversity: 0.2124
  Parameter Health: 22/22 healthy

Step 2500/5000
  Position: [-1.0000, 1.0000]
  Exploring: False
  Novelty: 0.0594
  Curiosity: 0.0008
  Total Reward: 0.1209
  Exploration Rate: 0.1820
  State Space Coverage: 25.0% (2500 states)
  Diversity: 0.2070
  Parameter Health: 22/22 healthy

Step 3000/5000
  Position: [-1.0000, 1.0000]
  Exploring: False
  Novelty: 0.0290
  Curiosity: 0.0004
  Total Reward: 0.0317
  Exploration Rate: 0.1646
  State Space Coverage: 30.0% (3000 states)
  Diversity: 0.1981
  Parameter Health: 22/22 healthy

Step 3500/5000
  Position: [-0.9812, 1.0000]
  Exploring: False
  Novelty: 0.0260
  Curiosity: 0.0005
  Total Reward: 0.0389
  Exploration Rate: 0.1490
  State Space Coverage: 35.0% (3500 states)
  Diversity: 0.2062
  Parameter Health: 22/22 healthy

Step 4000/5000
  Position: [-1.0000, 1.0000]
  Exploring: False
  Novelty: 0.0514
  Curiosity: 0.0009
  Total Reward: 0.1044
  Exploration Rate: 0.1348
  State Space Coverage: 40.0% (4000 states)
  Diversity: 0.1998
  Parameter Health: 22/22 healthy

Step 4500/5000
  Position: [-1.0000, 1.0000]
  Exploring: False
  Novelty: 0.0235
  Curiosity: 0.0003
  Total Reward: 0.0421
  Exploration Rate: 0.1220
  State Space Coverage: 45.0% (4500 states)
  Diversity: 0.2020
  Parameter Health: 22/22 healthy

Step 5000/5000
  Position: [-1.0000, 0.8319]
  Exploring: True
  Novelty: 0.0252
  Curiosity: 0.0001
  Total Reward: 0.0226
  Exploration Rate: 0.1104
  State Space Coverage: 50.0% (5000 states)
  Diversity: 0.2041
  Parameter Health: 22/22 healthy

================================================================================
EXPLORATION COMPLETE
================================================================================

Final Statistics:
  Total steps: 5000
  Final position: [-1.0000, 0.8319]
  States explored: 5000

Last 100 steps:
  Avg Novelty: 0.0333
  Avg Curiosity: 0.0002
  Avg Total Reward: 0.0611

Generating visualizations...
Saving timestamped versions to: exploration_runs/
Training metrics saved to exploration_metrics.png
Training metrics saved to exploration_runs\exploration_metrics_2025-12-18_17-33-29.png
State space exploration saved to exploration_state_space.png
State space exploration saved to exploration_runs\exploration_state_space_2025-12-18_17-33-29.png
Trajectory plot saved to exploration_trajectory.png
Trajectory plot saved to exploration_runs\exploration_trajectory_2025-12-18_17-33-29.png

Visualization complete!
Latest versions: exploration_*.png
Archived versions: exploration_runs/exploration_*_2025-12-18_17-33-29.png

Log saved to: exploration_runs/logs\run_log_2025-12-18_17-29-01.md
